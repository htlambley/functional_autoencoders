{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart 0: Getting started\n",
    "Welcome to the quickstart series for `functional_autoencoders`, a code package containing implementing\n",
    "\n",
    "- functional variational autoencoder (FVAE); and\n",
    "- functional autoencoder (FAE)\n",
    "\n",
    "in Python and [JAX](https://github.com/google/jax), based on our paper [*Autoencoders in Function Space*](https://arxiv.org/pdf/2408.01362). This series contains the following notebooks:\n",
    "\n",
    "1. [An Introduction to FVAE](./1_FVAE.ipynb)\n",
    "2. [An Introduction to FAE](./2_FAE.ipynb)\n",
    "3. [Custom Datasets](./3_Custom_Datasets.ipynb)\n",
    "4. [Custom Architectures](./4_Custom_Architectures.ipynb).\n",
    "\n",
    "If you want to get started as quickly as possible, you can jump straight into the first notebook. \n",
    "But we suggest that you first read the following two-minute introduction to `functional_autoencoders` so you can use the most appropriate model for your problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why use autoencoders in function space?\n",
    "\n",
    "Machine-learning methods for image data, e.g., convolutional neural networks, are usually formulated for a specific pixellation resolution.\n",
    "This means that all data must be provided on the same grid of pixels at training and inference time, even though the underlying images could in principle be stored at arbitrarily fine resolution.\n",
    "\n",
    "![Fixed-resolution ML algorithm can only accept data at the training resolution](./images/fixed-res_algorithms.png)\n",
    "\n",
    "Viewing the pixellated images as discrete representations of **functional data**, conceiving algorithms that operate directly on functions, and only then discretising, it is possible to use the same machine-learning model for training and inference across resolutions.\n",
    "\n",
    "![Diagram of conceiving of algorithms at function-space level and discretising](images/algorithms_on_fn_space.png)\n",
    "\n",
    "In scientific machine learning, this has led to significant interest in learnable mappings between function spaces such as [DeepONet](https://www.nature.com/articles/s42256-021-00302-5) and [neural operators](https://jmlr.org/papers/v24/21-1524.html).\n",
    "The `functional_autoencoders` package adopts this philosophy for autoencoders, which allows for:\n",
    "- training with data at any resolution, potentially with missing mesh points;\n",
    "- nonlinear dimension reduction for data provided at any resolution; and\n",
    "- encoding and decoding on different meshes, allowing for inpainting and superresolution.\n",
    "\n",
    "![FAE inpainting and superresolution](images/FAE_inpainting_superresolution.png)\n",
    "\n",
    "\n",
    "\n",
    "### Should I use FAE or FVAE?\n",
    "\n",
    "**Summary**: FAE is a good starting point that works \"out of the box\" for most data, whereas FVAE works only for specific types of data.\n",
    "\n",
    "![Flowchart to aid in deciding whether to use FVAE or FAE](images/fae_or_fvae_flowchart.png)\n",
    "\n",
    "FVAE uses variational inference to learn a probabilistic encoder and decoder.\n",
    "When the FVAE objective is well defined, it gives a very natural extension of VAEs to function space.\n",
    "But this training objective is only well defined for specific types of data, such as:\n",
    "- path distributions of stochastic differential equations (SDEs); and\n",
    "- Bayesian posterior distributions arising from Gaussian priors and \"nice\" forward models.\n",
    "\n",
    "You can read more about this in sections 2.3 and 3 of [the paper](https://arxiv.org/pdf/2408.01362).\n",
    "\n",
    "In contrast, FAE is not motivated as a probabilistic model: it is a regularised autoencoder that is well defined in function space. \n",
    "This makes FAE much more broadly applicable to many datasets in scientific machine learning (see section 4 of [the paper](https://arxiv.org/pdf/2408.01362)).\n",
    "Since FAE is not a probabilistic model, some extra work is needed to use FAE as a generative model for functional data, as we'll explain in the second quickstart notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "Start with either:\n",
    "1. [An Introduction to FVAE](./1_FVAE.ipynb); or\n",
    "2. [An Introduction to FAE](./2_FAE.ipynb).\n",
    "\n",
    "You can read these in any order, and they'll show you how to get started with a simple FVAE/FAE model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
